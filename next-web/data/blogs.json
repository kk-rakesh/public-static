[
  {
    "id": "1",
    "slug": "from-chaos-to-control",
    "title": "From Chaos To Control: Building systems that won't blink under pressure",
    "thumbnail": "/blogs/systems-pressure.png",
    "thumbnailTitle": "Designing Systems That Hold Under Pressure",
    "thumbnailCategory": "ENGINEERING",
    "tags": ["LLMs", "Systems"],
    "date": "January 22, 2026",
    "readTime": "10 min read time",
    "author": "Prakhar Gupta",
    "category": "Engineering",
    "subcategory": "AI",
    "introduction": "When your system handles millions of requests daily, even small inefficiencies compound into cascading failures, and a system that would fall over when we needed it most. This pushed us towards a complete re-architecture of our data orchestration layer at Ema, in line with the constant push to build systems that scale with our ambition. The goal became to design an intelligent ingestion platform that could handle anything, gracefully. We reimagined how data moves, how workloads are orchestrated, and how systems stay resilient under pressure.",
    "sections": [
      {
        "id": "more-power-problem",
        "title": "More power was the problem, not the solution",
        "content": "Our first instinct was to throw more hardware at the problem, but we quickly learned a hard lesson: adding power just created bigger traffic jams. The core issue was a fundamental flaw in our thinking.\n\nAs our initial design proposal stated, \"Scaling infrastructure does not translate into proportional throughput. We often face either resource underutilization or overload, leading to failures.\" Any fixed concurrency limit was doomed to fail; it would either be set too low, underutilizing our hardware, or too high, guaranteeing overload from a few heavy jobs.\n\nThe system treated a heavy task (like converting a 100-slide PPTX) and a light task (a 2-page text file) as equals. Heavy jobs would grab all the available resources, overwhelming the worker processes and causing them to crash. This not only blocked lighter tasks but often brought the entire pipeline to a halt. The old architecture tied our two main services, the Go-based Data Ingestion Service (DIS) and the Python-based Transform Service together with direct gRPC calls. This tight coupling created a single point of failure, amplifying crashes during high-load scenarios."
      },
      {
        "id": "separating-ask-act",
        "title": "Separating the 'ask' from the 'act'",
        "content": "The first major shift was decoupling \"asking for work\" from \"doing the work.\" We introduced RabbitMQ as a message broker between DIS and Transform Service. Instead of DIS directly calling Transform Service and waiting, it now drops a message into a queue and moves on.\n\nThis simple change was transformative. The Transform Service pulls work when it's ready, not when it's forced upon it. If Transform Service goes down, messages wait safely in the queue. DIS doesn't get blocked or crash because a downstream service is struggling."
      },
      {
        "id": "smart-bouncer",
        "title": "Every worker needed its own \"smart bouncer\"",
        "content": "We realized we couldn't just limit by the number of tasks. A single huge file could still bring down a worker, while dozens of small files might run fine. We needed a smarter approach.\n\nWe built a dynamic resource-based admission control system. Each worker now has a \"bouncer\" that checks available CPU and memory before accepting a new task. If resources are running low, the worker simply doesn't pull new work from the queue, letting it wait for another worker or for resources to free up.\n\nThis meant workers could self-regulate. They'd never bite off more than they could chew, and the system as a whole stayed healthy even when individual tasks were unpredictable."
      },
      {
        "id": "rosetta-stone",
        "title": "A \"rosetta stone\" for our services",
        "content": "Our old system had a confusing mix of job identifiers. DIS used one ID, Transform Service used another, and tracking a single file's journey through the system was nearly impossible.\n\nWe introduced a unified identifier called the Workload Identifier (WID). This single ID travels with every piece of work from the moment it enters the system until it's complete. Debugging went from a nightmare to straightforward. We could now trace any file's complete journey through logs, queues, and services with a single search.\n\nThe WID also enabled better monitoring and alerting. We could now set up dashboards that showed exactly where work was getting stuck or failing."
      }
    ]
  },
  {
    "id": "2",
    "slug": "intelligent-agent-orchestration-mcp",
    "title": "Intelligent Agent Orchestration Using MCP: Building autonomous systems with human oversight",
    "thumbnail": "/blogs/mcp-autonomous.png",
    "thumbnailTitle": "MCP for Autonomous Systems",
    "thumbnailCategory": "ENGINEERING",
    "tags": ["AI agents", "MCP", "LLMs"],
    "date": "January 7, 2026",
    "readTime": "15 min read time",
    "author": "Darshan Joshi",
    "category": "Engineering",
    "subcategory": "AI",
    "introduction": "As AI systems become more capable, the challenge shifts from \"can it do the task?\" to \"how do we maintain meaningful human control?\" The Model Context Protocol (MCP) offers a structured approach to building autonomous systems that remain accountable and transparent.",
    "sections": [
      {
        "id": "why-orchestration-matters",
        "title": "Why orchestration matters",
        "content": "Modern AI agents can perform complex multi-step tasks, but without proper orchestration, they become black boxes. We need systems that can explain their reasoning, pause for human approval when needed, and gracefully handle failures.\n\nOrchestration isn't just about managing tasks—it's about maintaining trust. When an AI system can clearly communicate what it's doing and why, humans can make informed decisions about when to intervene."
      },
      {
        "id": "mcp-architecture",
        "title": "The MCP architecture",
        "content": "MCP provides a standardized way for AI agents to interact with tools and data sources. At its core, it defines a protocol for context management, tool invocation, and result handling.\n\nThe architecture separates concerns cleanly: the agent focuses on reasoning and planning, while MCP handles the mechanics of execution. This separation makes systems easier to audit, test, and modify."
      },
      {
        "id": "human-in-loop",
        "title": "Human-in-the-loop patterns",
        "content": "We've identified several patterns for human oversight that balance autonomy with control. The \"checkpoint\" pattern pauses execution at critical decision points. The \"audit trail\" pattern logs all actions for later review. The \"escalation\" pattern automatically involves humans when confidence is low.\n\nThe key insight is that human oversight shouldn't slow down routine operations—it should focus attention where it matters most."
      },
      {
        "id": "practical-implementation",
        "title": "Practical implementation",
        "content": "Implementing MCP requires careful attention to error handling and state management. Agents must be able to resume from interruptions, handle partial failures, and maintain consistency across distributed operations.\n\nWe've found that explicit state machines work well for managing agent lifecycles. Each state transition is logged, making it easy to understand what happened and why."
      }
    ]
  },
  {
    "id": "3",
    "slug": "science-of-evaluating-ai-work",
    "title": "The Science of Evaluating AI Work: Measuring the performance of AI agents",
    "thumbnail": "/blogs/ai-evaluation.png",
    "thumbnailTitle": "AI Performance Evaluation",
    "thumbnailCategory": "RESEARCH",
    "tags": ["LLMs", "Evaluation", "Performance"],
    "date": "July 15, 2025",
    "readTime": "11 min read time",
    "author": "Mandar Joshi",
    "category": "Engineering",
    "subcategory": "AI",
    "introduction": "How do you measure success when AI systems perform open-ended tasks? Traditional metrics fall short when evaluating creative problem-solving, nuanced communication, or complex reasoning. We need new frameworks that capture what actually matters.",
    "sections": [
      {
        "id": "beyond-accuracy",
        "title": "Beyond accuracy metrics",
        "content": "Accuracy tells you if the answer was right, but not if the process was sound. For AI agents, the journey matters as much as the destination. Did the agent explore reasonable alternatives? Did it recognize uncertainty? Did it ask for clarification when needed?\n\nWe've developed multi-dimensional evaluation frameworks that assess reasoning quality, resource efficiency, and interaction patterns alongside traditional correctness metrics."
      },
      {
        "id": "human-evaluation",
        "title": "Human evaluation at scale",
        "content": "Some aspects of AI performance can only be judged by humans. But human evaluation is expensive and inconsistent. We've developed techniques for calibrating human evaluators, sampling efficiently, and combining human judgments with automated metrics.\n\nThe key is identifying which aspects truly require human judgment and automating everything else. This focuses expensive human attention where it creates the most value."
      },
      {
        "id": "continuous-monitoring",
        "title": "Continuous monitoring in production",
        "content": "Evaluation doesn't stop at deployment. Production systems need continuous monitoring to detect drift, catch edge cases, and identify improvement opportunities.\n\nWe've built dashboards that track not just outcomes but behavioral patterns. When an agent starts behaving differently—even if results seem fine—that's often an early warning sign worth investigating."
      },
      {
        "id": "benchmarks-limitations",
        "title": "Benchmarks and their limitations",
        "content": "Benchmarks provide standardized comparisons but can create perverse incentives. When you optimize for a benchmark, you often sacrifice real-world performance.\n\nWe advocate for benchmark diversity and regular refresh cycles. No single benchmark should dominate evaluation, and benchmarks should evolve to remain challenging and relevant."
      }
    ]
  },
  {
    "id": "4",
    "slug": "real-time-data-pipelines",
    "title": "Real-Time Data Pipelines: Processing millions of events per second",
    "thumbnail": "/blogs/data-pipelines.png",
    "thumbnailTitle": "Real-Time Data Processing",
    "thumbnailCategory": "ENGINEERING",
    "tags": ["Data", "Streaming", "Kafka"],
    "date": "December 18, 2025",
    "readTime": "12 min read time",
    "author": "Arun Kumar",
    "category": "Engineering",
    "subcategory": "Data",
    "introduction": "When every millisecond counts, traditional batch processing falls short. We rebuilt our data infrastructure to handle real-time streams at scale, learning valuable lessons about backpressure, exactly-once semantics, and graceful degradation along the way.",
    "sections": [
      {
        "id": "why-real-time",
        "title": "Why real-time matters",
        "content": "In modern applications, users expect instant feedback. Whether it's fraud detection, recommendation systems, or live dashboards, the value of data diminishes rapidly with time.\n\nWe found that moving from batch to streaming reduced our insight latency from hours to seconds, fundamentally changing what was possible for our product teams."
      },
      {
        "id": "architecture-choices",
        "title": "Architecture choices",
        "content": "We evaluated several streaming platforms before settling on Apache Kafka as our backbone. Its durability guarantees, partitioning model, and ecosystem made it the right choice for our scale.\n\nThe key insight was treating Kafka as more than a message queue—it became our source of truth, with consumers replaying history when needed."
      }
    ]
  },
  {
    "id": "5",
    "slug": "securing-api-endpoints",
    "title": "Securing API Endpoints: A defense-in-depth approach",
    "thumbnail": "/blogs/api-security.png",
    "thumbnailTitle": "API Security Best Practices",
    "thumbnailCategory": "SECURITY",
    "tags": ["Security", "API", "Authentication"],
    "date": "November 30, 2025",
    "readTime": "9 min read time",
    "author": "Priya Sharma",
    "category": "Security",
    "subcategory": "API",
    "introduction": "APIs are the front door to your application. Every endpoint is a potential attack vector. We share our journey implementing multiple layers of security without sacrificing developer experience or performance.",
    "sections": [
      {
        "id": "authentication-patterns",
        "title": "Authentication patterns",
        "content": "We moved from simple API keys to JWT-based authentication with short-lived tokens. The migration was complex but dramatically improved our security posture.\n\nKey decisions included token rotation strategies, refresh token handling, and graceful degradation when auth services are unavailable."
      },
      {
        "id": "rate-limiting",
        "title": "Intelligent rate limiting",
        "content": "Naive rate limiting by IP address fails in the modern world of shared IPs and determined attackers. We implemented adaptive rate limiting that considers user behavior patterns.\n\nThe system learns normal usage patterns and flags anomalies, allowing legitimate traffic while blocking abuse."
      }
    ]
  },
  {
    "id": "6",
    "slug": "building-design-systems",
    "title": "Building Design Systems: Creating consistency at scale",
    "thumbnail": "/blogs/design-systems.png",
    "thumbnailTitle": "Design Systems Architecture",
    "thumbnailCategory": "DESIGN",
    "tags": ["Design", "UI", "Components"],
    "date": "October 22, 2025",
    "readTime": "8 min read time",
    "author": "Kavya Nair",
    "category": "Design",
    "subcategory": "UI",
    "introduction": "As our product grew, so did inconsistency. Different teams built similar components in different ways. We invested in a design system that became the foundation for all our interfaces.",
    "sections": [
      {
        "id": "component-architecture",
        "title": "Component architecture",
        "content": "We structured our components in layers: primitives, composites, and patterns. Primitives are the atomic building blocks—buttons, inputs, typography. Composites combine primitives into reusable units.\n\nThis layering made it easy to maintain consistency while allowing flexibility where needed."
      },
      {
        "id": "documentation-adoption",
        "title": "Documentation and adoption",
        "content": "A design system is only valuable if people use it. We invested heavily in documentation, including live examples, usage guidelines, and migration paths from legacy components.\n\nAdoption grew organically once teams saw the time savings and quality improvements."
      }
    ]
  },
  {
    "id": "7",
    "slug": "kubernetes-production-lessons",
    "title": "Kubernetes in Production: Hard lessons learned",
    "thumbnail": "/blogs/kubernetes.png",
    "thumbnailTitle": "Kubernetes Production Guide",
    "thumbnailCategory": "DEVOPS",
    "tags": ["Kubernetes", "DevOps", "Infrastructure"],
    "date": "September 15, 2025",
    "readTime": "14 min read time",
    "author": "Rahul Menon",
    "category": "Engineering",
    "subcategory": "DevOps",
    "introduction": "Kubernetes promised us container orchestration nirvana. The reality was more nuanced. Here are the hard lessons we learned running Kubernetes in production for three years.",
    "sections": [
      {
        "id": "resource-management",
        "title": "Resource management pitfalls",
        "content": "Setting resource requests and limits correctly is deceptively difficult. Too low, and pods get evicted. Too high, and you waste expensive infrastructure.\n\nWe developed tooling to analyze actual usage patterns and recommend appropriate values, reducing our infrastructure costs by 30%."
      },
      {
        "id": "networking-complexity",
        "title": "Networking complexity",
        "content": "Kubernetes networking is powerful but complex. Service meshes add another layer of abstraction that can be both helpful and confusing.\n\nWe standardized on a simplified networking model that met our needs without the full complexity of a service mesh."
      }
    ]
  },
  {
    "id": "8",
    "slug": "testing-strategies-microservices",
    "title": "Testing Strategies for Microservices: Beyond unit tests",
    "thumbnail": "/blogs/testing.png",
    "thumbnailTitle": "Microservices Testing",
    "thumbnailCategory": "ENGINEERING",
    "tags": ["Testing", "Microservices", "Quality"],
    "date": "August 8, 2025",
    "readTime": "10 min read time",
    "author": "Sneha Patel",
    "category": "Engineering",
    "subcategory": "Testing",
    "introduction": "Unit tests alone can't catch the bugs that matter most in distributed systems. We developed a comprehensive testing strategy that gives us confidence to deploy multiple times per day.",
    "sections": [
      {
        "id": "contract-testing",
        "title": "Contract testing",
        "content": "When services communicate via APIs, both sides need to agree on the contract. We use Pact for consumer-driven contract testing, catching breaking changes before they reach production.\n\nThis approach shifted integration bugs left, finding them in CI rather than staging or production."
      },
      {
        "id": "chaos-engineering",
        "title": "Chaos engineering",
        "content": "We regularly inject failures into our production systems to verify our resilience. Network partitions, service outages, and resource exhaustion are all part of our chaos experiments.\n\nThe practice has uncovered numerous hidden dependencies and failure modes that traditional testing missed."
      }
    ]
  },
  {
    "id": "9",
    "slug": "observability-distributed-systems",
    "title": "Observability in Distributed Systems: Seeing the invisible",
    "thumbnail": "/blogs/observability.png",
    "thumbnailTitle": "Distributed Observability",
    "thumbnailCategory": "ENGINEERING",
    "tags": ["Observability", "Monitoring", "Tracing"],
    "date": "July 1, 2025",
    "readTime": "13 min read time",
    "author": "Vikram Singh",
    "category": "Engineering",
    "subcategory": "Operations",
    "introduction": "You can't fix what you can't see. In distributed systems, understanding behavior requires more than logs and metrics. We built an observability platform that makes the invisible visible.",
    "sections": [
      {
        "id": "three-pillars",
        "title": "The three pillars",
        "content": "Logs tell you what happened. Metrics tell you how much. Traces tell you where. Together, they provide a complete picture of system behavior.\n\nWe invested in correlating all three, allowing engineers to seamlessly move between views when investigating issues."
      },
      {
        "id": "cost-management",
        "title": "Managing observability costs",
        "content": "Observability data grows faster than your systems. Without careful management, costs can spiral out of control.\n\nWe implemented intelligent sampling, tiered storage, and automatic data lifecycle policies to keep costs manageable while maintaining visibility."
      }
    ]
  }
]
